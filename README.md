# problem-2

## Install

The shell commands mentioned below install `pipenv`, clone the repository, and setup a Python 3.7 virtual environment along with installing all the Python packages.

```sh
pip install pipenv
git clone https://github.com/saru95/problem-1; cd problem-1/
pipenv install; pipenv shell
```

## Execution

1. Specify files via command line arguments:

```sh
$ python script.py <TSV_File_Name> <Source_Code_File_1> <Source_Code_File_2> ...

# eg. python script.py result file1.c file2.c file3.c ....
```

2. Specify directory via prompt:

```sh
$ python script.py <TSV_File_Name>

  Enter the directory path of source code files: <path_to_directory_with_source_code_files>

# eg. python script.py result
# Enter the directory path of source code files: ./dataset/
# ...
```

## Process

1. The script first uses the C grammar (in `tokenizer/grammar/C.g4`) to generate a lexer in Python using <a href="http://www.antlr.org/">antr4</a>.

2. The script then reads all the specified source code files, extracts tokens from them and generates all the 
possible token sequences (within a file). This information is cached using a dictionary.

3. All the token sequences with length or count as 1 are removed as those sequences would end up given us a score of 0. In addition to this, all the token sequences that do not exist in (or are not shared across) all the files are removed from the cache.

4. In the end, the report is generated by reverse-sorting the cache by the scores of the token sequences.

## Corner Cases

1. As we increase the number of source files, the memory used by the script to maintain the cache of tokens starts exploding. This can be handled by using a persistent database to store the information. In my case, even though the memory shoots up till 15GB (in case of 100 files), the script tries to minimize the memory footprint wherever possible.

2. 


## Samples

A dataset accompanies the code. `/dataset` has sub-folders based on the number of files, the script can be tested on.

1. For 2 files:
    * Execution Command: 
    ```sh
    python script.py tsv_file_name
    Enter the directory path of source code files: ./dataset/two_files
    ```
    * The generated file is present in `/Samples/two_files.tsv`.

2. For 10 files:
    * Execution Command: 
    ```sh
    python script.py tsv_file_name
    Enter the directory path of source code files: ./dataset/ten_files
    ```
    * The generated file is present in `/Samples/ten_files.tsv`.

3. For 50 files:
    * Execution Command: 
    ```sh
    python script.py tsv_file_name
    Enter the directory path of source code files: ./dataset/fifty_files
    ```
    * The generated file is present in `/Samples/fifty_files.tsv`.

4. For 100 files:
    * Execution Command: 
    ```sh
    python script.py tsv_file_name
    Enter the directory path of source code files: ./dataset/hundred_files
    ```
    * The generated file is present in `/Samples/hundred_files.tsv`.


## Issues

CSVs are bad at handling values with commas within them (<a href="https://stackoverflow.com/a/4618007/3301488">src</a>). Even after enclosing the value inside double-quotes, CSV representation is not ideal. To mitigate this, the script supports a function called `write_to_tsv` which writes to a TSV file instead. TSV files are fairly easy to parse similar to CSVs.