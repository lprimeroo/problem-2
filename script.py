import sys
from antlr4 import *
from tokenizer import *
import glob
import itertools
from collections import defaultdict
from os.path import join
from math import log2

class TokenSequenceLCS():

  def __init__(self, files):
    self.files = files
    self.tokens_record = []
    self.result = []
    self.sequence_counts = defaultdict(lambda: 0) # maintains a record of counts of the token sequences
    self.sequence_lengths = [] # maintains a record of the lengths of the token sequences

  def get_all_subsequences(self):
    '''
      This routine gets all the possible token sequences given a list of token and
      maintains a record of their count of occurence and length.
    '''
    l = len(self.tokens_record)
    for i in range(l):
      for j in range(i, l):
        self.sequence_counts[' '.join(self.tokens_record[i:j + 1])] += 1
        self.sequence_lengths.append([self.tokens_record[i:j + 1], len(self.tokens_record[i:j + 1])])

  def perform_lcs_calculation(self):
    self.get_all_subsequences()
    seen_set = set()
    for record in self.sequence_lengths:
      if ' '.join(record[0]) in seen_set: continue
      count = self.sequence_counts[' '.join(record[0])]
      if count == 1 or record[1] == 1: continue
      seen_set.add(' '.join(record[0]))
      score = log2(record[1]) * log2(count)
      self.result.append([score, record[1], count, record[0]])
    self.result.sort(key=lambda x: x[0], reverse=True)
      # self.result += f'{score}\t{record[1]}\t{count}\t{}\n'

  def perform_tokenizing(self):
    '''
      This routine extracts the tokens from all the files using the lexer
      generated by antlr4 using C's grammar in grammar/C.g4. This routine also
      initiates the perform_lcs_calculation method.
    '''
    for file in self.files:
      contents = FileStream(file)
      lexer = CLexer(contents)
      token_stream = CommonTokenStream(lexer)
      token_stream.fill() # read till EOF
      tokens = [token.text for token in token_stream.tokens][:-1]
      self.tokens_record.append(tokens)
    self.tokens_record = list(itertools.chain(*self.tokens_record)) # combine tokens of all the files into one database
    self.perform_lcs_calculation()

  def write_to_tsv(self, filename):
    '''
      This routine write's the result to a tab-separated file.
    '''
    with open(f'{filename}.tsv', 'w') as file:
      file.write('score\ttokens\tcount\tsequence\n')
      for line in self.result:
        file.write(f'{line[0]}\t{line[1]}\t{line[2]}\t{line[3]}\n')

if __name__ == '__main__':
  files = []
  tsv_name = sys.argv[1]
  if len(sys.argv) > 2:
    files = sys.argv
    files.pop(0); files.pop(0)
  else:
    path = input('Enter the directory path of source code files: ')
    files = glob.glob(join(path, "*.c"))

  t = TokenSequenceLCS(files)
  t.perform_tokenizing()
  t.write_to_tsv(filename=tsv_name)